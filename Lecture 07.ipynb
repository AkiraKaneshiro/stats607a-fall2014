{
 "metadata": {
  "name": "",
  "signature": "sha256:e803b5a14fc25153faf9a36606c1a7001ea05b244060da1ca106b48c4d1b5c94"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Today we'll cover:\n",
      "\n",
      "1. [Optimization and root finding](#Optimization-and-root-finding)\n",
      "2. [Special functions](#Special-functions)\n",
      "3. [Statistical functions](#Statistical-functions)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Optimization and root finding"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import linalg as LA"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import minimize, rosen"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us run a simple [derivative free optimization](http://dx.doi.org/10.1137/1.9780898718768) method, called the [Nelder-Mead method](http://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method), starting from the above point `x0`. The function we will minimize is a popular test case in the optimization literature: the [Rosenbrock function](http://en.wikipedia.org/wiki/Rosenbrock_function)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = minimize(rosen, x0, method='nelder-mead',\n",
      "               options={'xtol': 1e-8, 'disp': True})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.000000\n",
        "         Iterations: 339\n",
        "         Function evaluations: 571\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res.x  # print the minimizer found"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.  1.  1.  1.  1.]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res  # res is an object with many fields"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  status: 0\n",
        "    nfev: 571\n",
        " success: True\n",
        "     fun: 4.8611534334221152e-17\n",
        "       x: array([ 1.,  1.,  1.,  1.,  1.])\n",
        " message: 'Optimization terminated successfully.'\n",
        "     nit: 339\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us try another deriviative free method called [Powell's Method](http://dx.doi.org/10.1093/comjnl/7.2.155). We will again use the Rosenbrock function for testing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = minimize(rosen, x0, method='powell',\n",
      "               options={'xtol': 1e-8, 'disp': True})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.000000\n",
        "         Iterations: 19\n",
        "         Function evaluations: 1622\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res.x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.  1.  1.  1.  1.]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res  # the structure of the returned objects depends on which method was used"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  status: 0\n",
        " success: True\n",
        "   direc: array([[  1.59217495e-06,   1.43301633e-06,   4.65187663e-06,\n",
        "          9.58520273e-06,   2.09783685e-05],\n",
        "       [ -5.72229228e-04,  -8.96827101e-04,  -2.05165897e-03,\n",
        "         -4.12621496e-03,  -7.47022249e-03],\n",
        "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
        "          0.00000000e+00,   0.00000000e+00],\n",
        "       [ -6.33752967e-03,  -3.93431060e-03,  -1.22576639e-03,\n",
        "          4.95313151e-05,   1.18066443e-04],\n",
        "       [  3.39029104e-08,   7.58841519e-08,   1.38160426e-07,\n",
        "          2.56876108e-07,   5.19973682e-07]])\n",
        "    nfev: 1622\n",
        "     fun: 2.0864733021357032e-21\n",
        "       x: array([ 1.,  1.,  1.,  1.,  1.])\n",
        " message: 'Optimization terminated successfully.'\n",
        "     nit: 19\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have so far use deriviative free or zero-order optimization methods. These methods only require function evaluations but can be slow to converge. Let us now look at first-order optimization methods, i.e. methods that evaluate derivatives of the function being optimized."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import rosen_der  # import the derivative of the Rosenbrock function"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "direction = np.random.randn(len(x0)); direction = direction/LA.norm(direction)  # choose a random unit vector"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print direction, \" has norm \", LA.norm(direction)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-0.73343437  0.32505413  0.41786723 -0.14531248 -0.40085545]  has norm  1.0\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eps = 1e-5  # a small number"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rosen_change = rosen(x0 + eps * direction) - rosen(x0)  # actual change in the function in a given direction"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "approx_change = rosen_der(x0).dot(eps * direction)  # approx change using first order expansion"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"actual change = \", rosen_change, \"\\nchange using 1st order Taylor expansion =\", approx_change"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "actual change =  -0.00723341948731 \n",
        "change using 1st order Taylor expansion = -0.00723348286858\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us now minimize the Rosenbrock function using the [BFGS method](http://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm). Note how we supply the derivative using the `jac` (for Jacobian) keyword argument."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "               options={'disp': True})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.000000\n",
        "         Iterations: 52\n",
        "         Function evaluations: 64\n",
        "         Gradient evaluations: 64\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res.x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.  1.  1.  1.  1.]\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res  # note that hess_inv is an approximation to the Hessian computed internally by BFGS"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   status: 0\n",
        "  success: True\n",
        "     njev: 64\n",
        "     nfev: 64\n",
        " hess_inv: array([[ 0.00742784,  0.01239067,  0.02361262,  0.04673885,  0.09348604],\n",
        "       [ 0.01239067,  0.02483023,  0.04731168,  0.09365448,  0.18731921],\n",
        "       [ 0.02361262,  0.04731168,  0.09490941,  0.18787576,  0.37577275],\n",
        "       [ 0.04673885,  0.09365448,  0.18787576,  0.3768795 ,  0.75378359],\n",
        "       [ 0.09348604,  0.18731921,  0.37577275,  0.75378359,  1.51262571]])\n",
        "      fun: 1.0115586212338213e-18\n",
        "        x: array([ 1.,  1.,  1.,  1.,  1.])\n",
        "  message: 'Optimization terminated successfully.'\n",
        "      jac: array([  4.35961711e-10,   3.31877303e-09,  -3.38990125e-08,\n",
        "         3.45087985e-08,  -9.65725278e-09])\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another first order method provided by `minimize` is the (Polak\u2013Ribi\u00e8re) [Conjugate Gradient (CG) method](http://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = minimize(rosen, x0, method='CG', jac=rosen_der,\n",
      "               options={'disp': True})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.000000\n",
        "         Iterations: 174\n",
        "         Function evaluations: 310\n",
        "         Gradient evaluations: 310\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res.x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.99999985  0.99999971  0.99999943  0.99999888  0.99999774]\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we will use a second-order optimization method, i.e. a method that uses Hessian evaluations. We will first import the Hessian of the Rosenbrock function from the `optimize` subpackage."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import rosen_hess"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us check if the Hessian works as expected in a second-order Taylor expansion."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "approx_change2 = rosen_der(x0).dot(eps * direction) + (eps * direction).dot(rosen_hess(x0)).dot(eps * direction)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"actual change = \", rosen_change, \"\\nchange using 2nd order Taylor approx. =\", approx_change2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "actual change =  -0.00723341948731 \n",
        "change using 2nd order Taylor approx. = -0.00723335610459\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will now use the line search Newton-CG method (also called [truncated Newton method](http://www.neos-guide.org/content/truncated-newton-methods))."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = minimize(rosen, x0, method='Newton-CG',\n",
      "               jac=rosen_der, hess=rosen_hess,  # now we supply both Jacobian as well as Hessian\n",
      "               options={'xtol': 1e-8, 'disp': True})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.000000\n",
        "         Iterations: 24\n",
        "         Function evaluations: 33\n",
        "         Gradient evaluations: 56\n",
        "         Hessian evaluations: 24\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res.x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.          1.          1.          0.99999999  0.99999999]\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us end this section by using one of the root finding functions in `scipy.optimize`.\n",
      "\n",
      "We will compute the [Omega constant](http://en.wikipedia.org/wiki/Omega_constant), namely the unique real number $\\Omega$ that satisifes the equality $\\Omega e^\\Omega = 1$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def self_times_exp(x):\n",
      "    return x*math.exp(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import bisect"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Omega = bisect(lambda x: self_times_exp(x) - 1, 0, 1)  # find the root of x*exp(x) - 1 using bisection on the interval [0, 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print Omega"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.56714329041\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print Omega * math.exp(Omega)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.0\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Special functions"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Statistical functions"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}