{
 "metadata": {
  "name": "",
  "signature": "sha256:0e869af219322321261d0f8e4805513d8534b335237d0ce26a2b7608e4ccde23"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Today we'll cover:\n",
      "\n",
      "1. [Optimization and root finding](#Optimization-and-root-finding)\n",
      "2. [Special functions](#Special-functions)\n",
      "3. [Statistical functions](#Statistical-functions)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Optimization and root finding"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import linalg as LA"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import minimize, rosen"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us run a simple [derivative free optimization](http://dx.doi.org/10.1137/1.9780898718768) method, called the [Nelder-Mead method](http://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method), starting from the above point `x0`. The function we will minimize is a popular test case in the optimization literature: the [Rosenbrock function](http://en.wikipedia.org/wiki/Rosenbrock_function)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = minimize(rosen, x0, method='nelder-mead',\n",
      "               options={'xtol': 1e-8, 'disp': True})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.000000\n",
        "         Iterations: 339\n",
        "         Function evaluations: 571\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res.x  # print the minimizer found"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.  1.  1.  1.  1.]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res  # res is an object with many fields"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  status: 0\n",
        "    nfev: 571\n",
        " success: True\n",
        "     fun: 4.8611534334221152e-17\n",
        "       x: array([ 1.,  1.,  1.,  1.,  1.])\n",
        " message: 'Optimization terminated successfully.'\n",
        "     nit: 339\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us try another deriviative free method called [Powell's Method](http://dx.doi.org/10.1093/comjnl/7.2.155). We will again use the Rosenbrock function for testing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = minimize(rosen, x0, method='powell',\n",
      "               options={'xtol': 1e-8, 'disp': True})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.000000\n",
        "         Iterations: 19\n",
        "         Function evaluations: 1622\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res.x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.  1.  1.  1.  1.]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res  # the structure of the returned objects depends on which method was used"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  status: 0\n",
        " success: True\n",
        "   direc: array([[  1.59217495e-06,   1.43301633e-06,   4.65187663e-06,\n",
        "          9.58520273e-06,   2.09783685e-05],\n",
        "       [ -5.72229228e-04,  -8.96827101e-04,  -2.05165897e-03,\n",
        "         -4.12621496e-03,  -7.47022249e-03],\n",
        "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
        "          0.00000000e+00,   0.00000000e+00],\n",
        "       [ -6.33752967e-03,  -3.93431060e-03,  -1.22576639e-03,\n",
        "          4.95313151e-05,   1.18066443e-04],\n",
        "       [  3.39029104e-08,   7.58841519e-08,   1.38160426e-07,\n",
        "          2.56876108e-07,   5.19973682e-07]])\n",
        "    nfev: 1622\n",
        "     fun: 2.0864733021357032e-21\n",
        "       x: array([ 1.,  1.,  1.,  1.,  1.])\n",
        " message: 'Optimization terminated successfully.'\n",
        "     nit: 19\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have so far use deriviative free or zero-order optimization methods. These methods only require function evaluations but can be slow to converge. Let us now look at first-order optimization methods, i.e. methods that evaluate derivatives of the function being optimized."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import rosen_der  # import the derivative of the Rosenbrock function"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "direction = np.random.randn(len(x0)); direction = direction/LA.norm(direction)  # choose a random unit vector"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print direction, \" has norm \", LA.norm(direction)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.69843784  0.32738821  0.44067682 -0.26009182  0.37836189]  has norm  1.0\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eps = 1e-5  # a small number"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rosen_change = rosen(x0 + eps * direction) - rosen(x0)  # actual change in the function in a given direction"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "approx_change = rosen_der(x0).dot(eps * direction)  # approx change using first order expansion"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"actual change = \", rosen_change, \"\\nchange using 1st order Taylor expansion =\", approx_change"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "actual change =  -0.00608757088639 \n",
        "change using 1st order Taylor expansion = -0.00608762848869\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us now minimize the Rosenbrock function using the [BFGS method](http://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm). Note how we supply the derivative using the `jac` (for Jacobian) keyword argument."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "               options={'disp': True})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.000000\n",
        "         Iterations: 52\n",
        "         Function evaluations: 64\n",
        "         Gradient evaluations: 64\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res.x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.  1.  1.  1.  1.]\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res  # note that hess_inv is an approximation to the Hessian computed internally by BFGS"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   status: 0\n",
        "  success: True\n",
        "     njev: 64\n",
        "     nfev: 64\n",
        " hess_inv: array([[ 0.00742784,  0.01239067,  0.02361262,  0.04673885,  0.09348604],\n",
        "       [ 0.01239067,  0.02483023,  0.04731168,  0.09365448,  0.18731921],\n",
        "       [ 0.02361262,  0.04731168,  0.09490941,  0.18787576,  0.37577275],\n",
        "       [ 0.04673885,  0.09365448,  0.18787576,  0.3768795 ,  0.75378359],\n",
        "       [ 0.09348604,  0.18731921,  0.37577275,  0.75378359,  1.51262571]])\n",
        "      fun: 1.0115586212338213e-18\n",
        "        x: array([ 1.,  1.,  1.,  1.,  1.])\n",
        "  message: 'Optimization terminated successfully.'\n",
        "      jac: array([  4.35961711e-10,   3.31877303e-09,  -3.38990125e-08,\n",
        "         3.45087985e-08,  -9.65725278e-09])\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another first order method provided by `minimize` is the (Polak\u2013Ribi\u00e8re) [Conjugate Gradient (CG) method](http://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = minimize(rosen, x0, method='CG', jac=rosen_der,\n",
      "               options={'disp': True})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.000000\n",
        "         Iterations: 174\n",
        "         Function evaluations: 310\n",
        "         Gradient evaluations: 310\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res.x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.99999985  0.99999971  0.99999943  0.99999888  0.99999774]\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we will use a second-order optimization method, i.e. a method that uses Hessian evaluations. We will first import the Hessian of the Rosenbrock function from the `optimize` subpackage."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import rosen_hess"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us check if the Hessian works as expected in a second-order Taylor expansion."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "approx_change2 = rosen_der(x0).dot(eps * direction) + (eps * direction).dot(rosen_hess(x0)).dot(eps * direction)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"actual change = \", rosen_change, \"\\nchange using 2nd order Taylor approx. =\", approx_change2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "actual change =  -0.00608757088639 \n",
        "change using 2nd order Taylor approx. = -0.00608751328405\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will now use the line search Newton-CG method (also called [truncated Newton method](http://www.neos-guide.org/content/truncated-newton-methods))."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = minimize(rosen, x0, method='Newton-CG',\n",
      "               jac=rosen_der, hess=rosen_hess,  # now we supply both Jacobian as well as Hessian\n",
      "               options={'xtol': 1e-8, 'disp': True})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 0.000000\n",
        "         Iterations: 24\n",
        "         Function evaluations: 33\n",
        "         Gradient evaluations: 56\n",
        "         Hessian evaluations: 24\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print res.x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.          1.          1.          0.99999999  0.99999999]\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us end this section by using one of the root finding functions in `scipy.optimize`.\n",
      "\n",
      "We will compute the [Omega constant](http://en.wikipedia.org/wiki/Omega_constant), namely the unique real number $\\Omega$ that satisifes the equality $\\Omega e^\\Omega = 1$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def self_times_exp(x):\n",
      "    return x*math.exp(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import bisect"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Omega = bisect(lambda x: self_times_exp(x) - 1, 0, 1)  # find the root of x*exp(x) - 1 using bisection on the interval [0, 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print Omega"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.56714329041\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print Omega * math.exp(Omega)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.0\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another useful function is `scipy.optimize.golden` that implements the [Golden Section Search method](http://en.wikipedia.org/wiki/Golden_section_search) due to the statistician [Jack Kiefer](http://en.wikipedia.org/wiki/Jack_Kiefer_%28statistician%29). It searches for the minimum of a univariate function on an interval where there is a unique minimum. Each iteration reduces the interval to search in by a factor of $1/\\phi$ where $\\phi = \\frac{1+\\sqrt{5}}{2}$ is the [golden ratio](http://en.wikipedia.org/wiki/Golden_ratio)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import golden"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us now find the unique minimum of the function $f(x) = xe^x$ on the interval $[-2, 0]$. Since $f'(x) = (1+x)e^x$, we know that the minimum is achived at $x=-1$ but let us find that numerically."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print golden(lambda x: x*math.exp(x), brack=(-2, 0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-1.00000000199\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Special functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`scipy.special` provides many special mathematical functions. Almost all of them are universal functions and so can be applied to ndarrays elementwise. There are far too many of them for us to cover them in any sufficient detail (it is best to consult the [reference page](http://docs.scipy.org/doc/scipy/reference/special.html#module-scipy.special) to get a good idea of what's available).\n",
      "\n",
      "Here we simply find the Omega constant again via the [Lambert W function](http://en.wikipedia.org/wiki/Lambert_W_function) $W(z)$. It (or, to be more precise, its principal branch) is defined for $z \\ge -1/e$ as the number $W(z) \\ge -1$ such that $z = W(z) e^{W(z)}$. So, the Omega constant is nothing but $W(1)$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.special import lambertw"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print lambertw(1)  # returns a complex number"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(0.56714329041+0j)\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.allclose(Omega, lambertw(1))  # check if our value, computed using bisect, agrees with the above"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Statistical functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `stats` subpackage provides many discrete rvs (random variables) and continuous rvs. Discrete rvs are instances of the class [`scipy.stats.rv_discrete`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_discrete.html#scipy.stats.rv_discrete). Continuous rvs are instances of the class [`scipy.stats.rv_continuous`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_discrete.html#scipy.stats.rv_discrete)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cont_rvs = [d for d in dir(stats) if isinstance(getattr(stats, d), stats.rv_continuous)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "disc_rvs = [d for d in dir(stats) if isinstance(getattr(stats, d), stats.rv_discrete)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"There are %d continuous and %d discrete distributions available.\" % (len(cont_rvs), len(disc_rvs))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 86 continuous and 13 discrete distributions available.\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import expon  # import the exponential distribution"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "samples = expon.rvs(size=50); print samples  # draw 50 samples from the distribution"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.2039175   0.03245034  1.55441426  2.04430455  0.12239728  1.32603086\n",
        "  0.07832514  0.38565631  0.81711191  2.0153179   0.18550339  2.18946477\n",
        "  0.28777859  1.34589434  0.63863135  0.99573863  0.82131177  1.18582165\n",
        "  2.77989057  0.21875585  0.17726738  0.21281314  2.1758386   0.1189996\n",
        "  0.58385385  0.93423673  0.1001409   0.45280366  0.37409071  0.7758253\n",
        "  0.84443713  1.04710833  0.0894964   1.07844061  0.3698278   0.45159882\n",
        "  0.13093783  0.08704655  0.10912173  1.10917405  2.66472272  4.05521254\n",
        "  0.30283126  0.53908202  0.31565075  0.7436101   0.07886545  0.47421893\n",
        "  0.14366809  0.90874293]\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.mean(samples), expon.mean()  # check closeness of empirical mean with true mean"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.833567617008 1.0\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's print the standard deviation, variance and entropy of an exponential rv."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"SD = %f, Var = %f, Entropy = %f\" % (expon.std(), expon.var(), expon.entropy())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "SD = 1.000000, Var = 1.000000, Entropy = 1.000000\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print expon.pdf(0)  # p.d.f. at 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.0\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print expon.cdf(0)  # c.d.f. at 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print expon.ppf(.75)  # inverse cdf, useful to get percentiles (75th in this case)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.38629436112\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "quantiles = np.arange(.1, 1, .1); print quantiles\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.allclose(expon.cdf(expon.ppf(quantiles)), quantiles)  # check if ppf is indeed inverse cdf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 51,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "expon.expect(lambda x: x**2)  # expectation of the function x^2 under the expon p.d.f."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "2.0"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We know that $\\int x^k e^{-x} dx = k!$. Let's test if `expect` works as expected."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.special import factorial"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.allclose(factorial(np.arange(10)), np.array([expon.expect(lambda x: x**k) for k in range(10)]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since expectations of $x^k$ are just the (uncentered) moments, we could have also equivalently done the following."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.allclose(factorial(np.arange(10)), np.array([expon.moment(k) for k in range(10)]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `median()` method returns the median. Let us test it on 4 popular continuous distributions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rv_names = ['expon', 'norm', 'cauchy', 'uniform']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rv_objects = [getattr(stats, rv_name) for rv_name in rv_names]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[np.abs(rv.median() - rv.ppf(0.5)) <= 1e-8 for rv in rv_objects]  # median should be inverse cdf at 0.5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "[True, True, True, True]"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let us work with a discrete rv, say Poisson."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import poisson"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "samples = poisson.rvs(mu=1, size=50)  # note how we have to supply the mu parameter in this case (no default assumed)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print samples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0 1 2 2 2 0 1 1 1 3 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 0 3 2 2 0 1 1\n",
        " 0 2 0 1 2 3 0 1 0 0 0 0 0]\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.mean(samples), poisson(mu=1).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.82 1.0\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you don't like supplying the `mu` argument all the time, you can also create a \"frozen rv\" (see discussion [here](http://docs.scipy.org/doc/scipy/reference/tutorial/stats.html#freezing-a-distribution))."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poisson1 = poisson(mu=1)  # create frozen rv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p_mean, p_var, p_skew = poisson1.stats('mvs')  # get mean, variance and skewness"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Mean = %f, Var = %f, Skewness = %f\" % (p_mean, p_var, p_skew)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mean = 1.000000, Var = 1.000000, Skewness = 1.000000\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Poisson($\\mu = 1$) has p.m.f. $p(i) = e^{-1} \\frac{1}{i!}, i \\in \\{0, 1, 2, \\ldots\\}$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "known_pmf = np.exp(-1)*(1/factorial(np.arange(10)))  # note that factorial (imported from scipy.special) is a ufunc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.allclose(known_pmf, poisson1.pmf(np.arange(10)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 67,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 67
    }
   ],
   "metadata": {}
  }
 ]
}