{
 "metadata": {
  "name": "",
  "signature": "sha256:79a1d6a36a72c4e8ebefeda7d7950ace964b2388ea2aa2d81bb534187619d901"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this lecture, we will build an [inverted index](http://en.wikipedia.org/wiki/Inverted_index) using Hadoop. We will treat each line as a \"document\" and will build an index where, for every word that occurs in at least one document, we will output the list of documents it appears in. For us, document ids will simply be line numbers."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Getting data into HDFS"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Login to flux using:\n",
      "\n",
      "    ssh uniqname@flux-login.engin.umich.edu\n",
      "    \n",
      "You'll need your MToken and your password to login.\n",
      "\n",
      "Once you're logged into flux, download tetx from the Bible and all works for Shakespeare and put them in an directory `documents` using:\n",
      "\n",
      "    mkdir documents\n",
      "    cd documents\n",
      "    wget https://raw.githubusercontent.com/ambujtewari/stats607a-fall2014/master/homeworks/datasets/bible+shakespeare.txt\n",
      "    \n",
      "Put your local directory `documents` into HDFS.\n",
      "\n",
      "    cd ..\n",
      "    hdfs dfs -copyFromLocal documents documents"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The Mapper"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Put the following code in a file called `map.py`:\n",
      "\n",
      "    #!/usr/bin/env python\n",
      "\n",
      "    import sys\n",
      "\n",
      "    for line in sys.stdin:\n",
      "        split_line = line.split()\n",
      "        doc_id = split_line[0]\n",
      "        word_list = split_line[1:]\n",
      "        word_set = set([w for w in word_list if len(w) > 0])\n",
      "        for word in word_set:\n",
      "            print word + '\\t' + doc_id\n",
      "        \n",
      "Can you figure out what the mapper is doing?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The Reducer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Put the following code in a file called `reduce.py`.\n",
      "\n",
      "    #!/usr/bin/env python\n",
      "\n",
      "    import sys\n",
      "\n",
      "    kv = dict()\n",
      "\n",
      "    for line in sys.stdin:\n",
      "        word = line.split('\\t')[0]\n",
      "        doc_id = line.split('\\t')[1].strip()\n",
      "        if word not in kv.keys():\n",
      "            kv[word] = doc_id\n",
      "        else:\n",
      "            kv[word] = kv[word] + \",\" + doc_id\n",
      "\n",
      "    for word in sorted(kv.keys()):\n",
      "        print(word + '\\t' + kv[word])                        \n",
      "        \n",
      "Can you figure out what the reducer is doing?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Test you mapper and reducer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Make both `map.py` and `reduce.py` executable by typing:\n",
      "\n",
      "    chmod +x map.py reduce.py\n",
      "    \n",
      "Then type:\n",
      "\n",
      "    head -5 documents/bible+shakespeare.txt | ./map.py | sort | ./reduce.py\n",
      "    \n",
      "You should see:\n",
      "\n",
      "    890904\t1\n",
      "    and\t2,3,4,5\n",
      "    authorized\t1\n",
      "    be\t4\n",
      "    beginning\t2\n",
      "    bible\t1\n",
      "    created\t2\n",
      "    darkness\t3,5\n",
      "    deep\t3\n",
      "    divided\t5\n",
      "    earth\t2,3\n",
      "    face\t3\n",
      "    form\t3\n",
      "    from\t5\n",
      "    god\t2,3,4,5\n",
      "    good\t5\n",
      "    heaven\t2\n",
      "    holy\t1\n",
      "    in\t2\n",
      "    it\t5\n",
      "    james\t1\n",
      "    king\t1\n",
      "    let\t4\n",
      "    light\t4,5\n",
      "    moved\t3\n",
      "    of\t3\n",
      "    said\t4\n",
      "    saw\t5\n",
      "    spirit\t3\n",
      "    textfile\t1\n",
      "    that\t5\n",
      "    the\t2,3,5\n",
      "    there\t4\n",
      "    upon\t3\n",
      "    version\t1\n",
      "    void\t3\n",
      "    was\t3,4,5\n",
      "    waters\t3\n",
      "    without\t3"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Running the map-reduce job"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Set the shell variable `queue` to `stats607f14`:\n",
      "\n",
      "    queue=stats607f14\n",
      "    \n",
      "Then type:\n",
      "\n",
      "    yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
      "        -Dmapreduce.job.queuename=$queue -input documents -output inv_index \\\n",
      "        -mapper map.py -reducer reduce.py -file map.py -file reduce.py \\\n",
      "        -numReduceTasks 10 \\\n",
      "        -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n",
      "        \n",
      "on the flux login prompt. Once the job finishes, a directory `inv_index` will have been created in the HDFS file-system. Type:\n",
      "\n",
      "    hdfs dfs -ls inv_index\n",
      "    \n",
      "and you should see something like:\n",
      "\n",
      "     Found 11 items\n",
      "    -rw-r-----   3 tewaria hadoop          0 2014-10-21 19:59 inv_index/_SUCCESS\n",
      "    -rw-r-----   3 tewaria hadoop    1140109 2014-10-21 19:59 inv_index/part-00000\n",
      "    -rw-r-----   3 tewaria hadoop    1236767 2014-10-21 19:59 inv_index/part-00001\n",
      "    -rw-r-----   3 tewaria hadoop     901007 2014-10-21 19:59 inv_index/part-00002\n",
      "    -rw-r-----   3 tewaria hadoop     964835 2014-10-21 19:59 inv_index/part-00003\n",
      "    -rw-r-----   3 tewaria hadoop     810575 2014-10-21 19:59 inv_index/part-00004\n",
      "    -rw-r-----   3 tewaria hadoop    1058884 2014-10-21 19:59 inv_index/part-00005\n",
      "    -rw-r-----   3 tewaria hadoop     733607 2014-10-21 19:59 inv_index/part-00006\n",
      "    -rw-r-----   3 tewaria hadoop    1511587 2014-10-21 19:59 inv_index/part-00007\n",
      "    -rw-r-----   3 tewaria hadoop     646031 2014-10-21 19:59 inv_index/part-00008\n",
      "    -rw-r-----   3 tewaria hadoop     716431 2014-10-21 19:59 inv_index/part-00009\n",
      "\n",
      "You get a single merged file in your local directory by using the `getmerge` option.\n",
      "\n",
      "    hdfs dfs -getmerge inv_index inv_index.txt"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Examining the inverted index"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On the login prompt type:\n",
      "\n",
      "    sort inv_index.txt | tail\n",
      "    \n",
      "Do you see the word \"zwaggered\"? Which documents (line numbers) does it appear? Can you confirm that by looking in `bible+shakespeare.txt`?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}