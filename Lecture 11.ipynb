{
 "metadata": {
  "name": "",
  "signature": "sha256:6fee9c60b43d781c2d8dca8c27ab57e87021f0aa3c839fe614413704a1208337"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Getting data into HDFS"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Login to flux using:\n",
      "\n",
      "    ssh uniqname@flux-login.engin.umich.edu\n",
      "    \n",
      "You'll need your MToken and your password to login.\n",
      "\n",
      "Once you're logged into flux, download the following two ebooks:\n",
      "\n",
      "* [The Notebooks of Leonardo Da Vinci](http://www.gutenberg.org/cache/epub/5000/pg5000.txt)\n",
      "* [On the Origin of Species](http://www.gutenberg.org/cache/epub/2009/pg2009.txt)\n",
      "\n",
      "and put them in an directory `input` using:\n",
      "\n",
      "    mkdir input\n",
      "    cd input\n",
      "    wget http://www.gutenberg.org/cache/epub/5000/pg5000.txt\n",
      "    wget http://www.gutenberg.org/cache/epub/2009/pg2009.txt\n",
      "    \n",
      "Next, we will transfer over these files into the HDFS file system. We can see the contents of the file system by typing\n",
      "\n",
      "    hdfs dfs -ls\n",
      "\n",
      "Right now, there's nothing there so nothing will show up. Let us put our local directory `input` into HDFS.\n",
      "\n",
      "    cd\n",
      "    hdfs dfs -copyFromLocal input input\n",
      "    \n",
      "Now if you type\n",
      "\n",
      "    hdfs dfs -ls\n",
      "    \n",
      "you will see something like:\n",
      "\n",
      "    Found 1 items\n",
      "    drwxr-x---   - tewaria hadoop          0 2014-10-19 18:37 input\n",
      "    \n",
      "You can also list contents of the directory `input` that you just created on HDFS:\n",
      "\n",
      "    hdfs dfs -ls input\n",
      "    \n",
      "and you should see something like:\n",
      "\n",
      "    Found 2 items\n",
      "    -rw-r-----   3 tewaria hadoop    1292470 2014-10-19 18:37 input/pg2009.txt\n",
      "    -rw-r-----   3 tewaria hadoop    1423803 2014-10-19 18:37 input/pg5000.txt"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The Mapper"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Put the following code in a file called `map.py`:\n",
      "\n",
      "    #!/usr/bin/env python\n",
      "\n",
      "    import sys\n",
      "\n",
      "    word_counts = dict()\n",
      "\n",
      "    for line in sys.stdin:\n",
      "        tokenized = map(lambda c: c if c.isalpha() else ' ', line)\n",
      "        word_list = ''.join(tokenized).split(' ')\n",
      "        for word in word_list:\n",
      "            if len(word) > 0:\n",
      "                if word not in word_counts.keys():\n",
      "                    word_counts[word] = 1\n",
      "                else:\n",
      "                    word_counts[word] = word_counts[word] + 1\n",
      "\n",
      "    for word in sorted(word_counts.keys()):\n",
      "        print(word + '\\t' + str(word_counts[word]))\n",
      "        \n",
      "The job of the mapper is to output (word, count) key-value pairs. In the streaming mode, this is done by simply printing the word and count separated by a tab character. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The Reducer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Put the following code in a file called `reduce.py`.\n",
      "\n",
      "    #!/usr/bin/env python\n",
      "\n",
      "    import sys\n",
      "\n",
      "    kv = dict()\n",
      "\n",
      "    for line in sys.stdin:\n",
      "        key = line.split('\\t')[0]\n",
      "        val = int(line.split('\\t')[1])\n",
      "        if key not in kv.keys():\n",
      "            kv[key] = val\n",
      "        else:\n",
      "            kv[key] = kv[key] + val\n",
      "\n",
      "    for key in sorted(kv.keys()):\n",
      "        print(key + '\\t' + str(kv[key]))\n",
      "        \n",
      "The job of the reducer is to read the (word, count) key-value pairs coming from different mappers and combine them into final (word, count) pairs."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Running the map-reduce job"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Set the shell variable `queue` to `stats607f14`:\n",
      "\n",
      "    queue=stats607f14\n",
      "    \n",
      "Then type:\n",
      "\n",
      "    yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
      "        -Dmapreduce.job.queuename=$queue -input input -output output \\\n",
      "        -mapper map.py -reducer reduce.py -file map.py -file reduce.py \\\n",
      "        -numReduceTasks 10 \\\n",
      "        -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n",
      "        \n",
      "on the flux login prompt. You will see some output like:\n",
      "\n",
      "    14/10/19 20:00:08 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "    packageJobJar: [map.py, reduce.py] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.3.0-cdh5.0.2.jar] /tmp/streamjob8598563688854264729.jar tmpDir=null\n",
      "    14/10/19 20:00:09 INFO client.RMProxy: Connecting to ResourceManager at fladoop-rm.engin.umich.edu/10.164.5.157:8032\n",
      "    14/10/19 20:00:09 INFO client.RMProxy: Connecting to ResourceManager at fladoop-rm.engin.umich.edu/10.164.5.157:8032\n",
      "    14/10/19 20:00:10 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "    14/10/19 20:00:10 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "    14/10/19 20:00:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1411748166591_0613\n",
      "    14/10/19 20:00:10 INFO impl.YarnClientImpl: Submitted application application_1411748166591_0613\n",
      "    14/10/19 20:00:10 INFO mapreduce.Job: The url to track the job: http://fladoop-rm.engin.umich.edu:8088/proxy/application_1411748166591_0613/\n",
      "    14/10/19 20:00:10 INFO mapreduce.Job: Running job: job_1411748166591_0613\n",
      "    14/10/19 20:00:15 INFO mapreduce.Job: Job job_1411748166591_0613 running in uber mode : false\n",
      "    14/10/19 20:00:15 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "    14/10/19 20:00:25 INFO mapreduce.Job:  map 26% reduce 0%\n",
      "\n",
      "Once the job finishes, a directory `output` will have been created in the HDFS file-system. Type:\n",
      "\n",
      "    hdfs dfs -ls output\n",
      "    \n",
      "and you should see something like:\n",
      "\n",
      "    Found 11 items\n",
      "    -rw-r-----   3 tewaria hadoop          0 2014-10-19 20:01 output/_SUCCESS\n",
      "    -rw-r-----   3 tewaria hadoop      24236 2014-10-19 20:01 output/part-00000\n",
      "    -rw-r-----   3 tewaria hadoop      24132 2014-10-19 20:01 output/part-00001\n",
      "    -rw-r-----   3 tewaria hadoop      24190 2014-10-19 20:01 output/part-00002\n",
      "    -rw-r-----   3 tewaria hadoop      24309 2014-10-19 20:01 output/part-00003\n",
      "    -rw-r-----   3 tewaria hadoop      24336 2014-10-19 20:01 output/part-00004\n",
      "    -rw-r-----   3 tewaria hadoop      25462 2014-10-19 20:01 output/part-00005\n",
      "    -rw-r-----   3 tewaria hadoop      24231 2014-10-19 20:01 output/part-00006\n",
      "    -rw-r-----   3 tewaria hadoop      24635 2014-10-19 20:01 output/part-00007\n",
      "    -rw-r-----   3 tewaria hadoop      24950 2014-10-19 20:01 output/part-00008\n",
      "    -rw-r-----   3 tewaria hadoop      25173 2014-10-19 20:01 output/part-00009\n",
      "    \n",
      "You get a single merged file in your local directory by using the `getmerge` option.\n",
      "\n",
      "    hdfs dfs -getmerge output output.txt"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Finding the top 50 most frequent words"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On the login prompt type:\n",
      "\n",
      "    sort -nr -k2,2 output.txt | head -50"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}